{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mdn5SojThO5"
      },
      "source": [
        "# Data PreProcessing Sourced Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whQfK2suT223"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkzaXO__T0O7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynxDod0sVuEi"
      },
      "source": [
        "\n",
        "\n",
        "I understand that this code imports several important Python libraries for data handling and natural language processing (NLP):\n",
        "\n",
        "- I import pandas because it is a powerful and flexible library for handling structured data in Python. It provides efficient data structures like DataFrames that allow me to manipulate, clean, and analyze data in tabular form, which is essential for preparing datasets for machine learning or any data-driven task.\n",
        "\n",
        "- I bring in the re module to perform pattern matching and text manipulation using regular expressions. This is crucial for operations like searching, replacing, or extracting parts of strings based on complex rules, which speeds up text cleaning and normalization processes.\n",
        "\n",
        "- I also use nltk, a leading toolkit for natural language processing, along with its stopwords corpus and word_tokenize function. Based on my research, stopwords help me filter out very common words that typically don't contribute meaningful information in NLP tasks, improving model focus. The tokenizer breaks down text into individual words (tokens), which is a fundamental step in almost all NLP workflows.\n",
        "\n",
        "With these tools combined, I set a solid foundation for working with textual and tabular data effectively, ensuring I can preprocess, clean, and analyze datasets with confidence and precision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_G3Y64bT6NI"
      },
      "source": [
        "# Download Necessary NLTK resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7Sj8EDkT9oZ",
        "outputId": "b6b701ce-1c42-4905-da83-162f706dfa27"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghb6YADdVvl0"
      },
      "source": [
        "From my point of view, I included the calls to nltk.download('stopwords') and nltk.download('punkt') because, based on my research, these downloads are essential to ensure that the NLTK library has access to the necessary language resources.\n",
        "\n",
        "- The 'stopwords' corpus provides a predefined list of common words, such as \"the\", \"is\", and \"and\", that are typically removed in text preprocessing since they add little semantic value and can introduce noise in analysis or modeling tasks.\n",
        "\n",
        "- The 'punkt' tokenizer model is required for the word_tokenize function to work properly. It helps in splitting the raw text into meaningful tokens (words or punctuation), which is a crucial foundational step in natural language processing.\n",
        "\n",
        "Downloading these resources upfront is a best practice to avoid runtime errors and to ensure that my NLP pipeline runs smoothly and accurately on any environment where the code is executed.\n",
        "\n",
        "This is the resource that i used to gain some better undestanding of this concept : https://www.geeksforgeeks.org/nlp/removing-stop-words-nltk-python/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7H7wqphT_J5"
      },
      "source": [
        "# Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlTp7f2-UCbQ",
        "outputId": "e1aac673-34e6-4caf-bae8-152ddf8baf2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded dataset ‚Äî Rows: 41157, Columns: 6\n"
          ]
        }
      ],
      "source": [
        "file_path = 'Corona_NLP_train.csv'\n",
        "df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
        "print(f\"Loaded dataset ‚Äî Rows: {df.shape[0]}, Columns: {df.shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIEuLH_DV3fz"
      },
      "source": [
        "I used encoding='ISO-8859-1' in the pd.read_csv function to load the file \"Corona_NLP_train.csv\"showing that many CSV files, especially those with special or accented characters, may not be encoded in UTF-8 by default.\n",
        "\n",
        "ISO-8859-1 (also called Latin-1) is a widely supported encoding standard that covers Western European characters and is often necessary when UTF-8 causes decoding errors. This encoding choice ensures that the text is read correctly without throwing errors or misinterpreting characters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2J0mPI5UFH8"
      },
      "source": [
        "# Total word count before cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLjBeNAsUIe7",
        "outputId": "5409ddf7-905f-496c-b255-046f7a75d293"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Total Words: 1255301\n"
          ]
        }
      ],
      "source": [
        "original_word_count = df['OriginalTweet'].astype(str).apply(lambda x: len(x.split())).sum()\n",
        "print(f\"Original Total Words: {original_word_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OphUscZV62e"
      },
      "source": [
        "This line of code to calculate the total word count for the \"OriginalTweet\" column:\n",
        "\n",
        "- I first convert the column to string format to avoid errors if any entries are not strings.\n",
        "\n",
        "- Then, I apply a lambda function that splits each text entry by spaces and counts the resulting tokens, which represent words.\n",
        "\n",
        "- Summing across all rows gives me the total number of words in the entire column.\n",
        "\n",
        "This approach is commonly used and recommended because splitting a string by whitespace is a simple and accurate way to count words in text data, especially in tweet-like content where words are typically space-separated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y9xg1ZpUebW"
      },
      "source": [
        "# Preview messy/raw tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfNjpXazUUKD",
        "outputId": "0c448d8a-15de-4748-edac-896bd08068a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Sample of Noisy Tweets (First 5):\n",
            "\n",
            "Tweet 1: @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/iFz9FAn2Pa and https://t.co/xX6ghGFzCC and https://t.co/I2NlzdxNo8\n",
            "\n",
            "Tweet 2: advice Talk to your neighbours family to exchange phone numbers create contact list with phone numbers of neighbours schools employer chemist GP set up online shopping accounts if poss adequate supplies of regular meds but not over order\n",
            "\n",
            "Tweet 3: Coronavirus Australia: Woolworths to give elderly, disabled dedicated shopping hours amid COVID-19 outbreak https://t.co/bInCA9Vp8P\n",
            "\n",
            "Tweet 4: My food stock is not the only one which is empty...\r\r\n",
            "\r\r\n",
            "PLEASE, don't panic, THERE WILL BE ENOUGH FOOD FOR EVERYONE if you do not take more than you need. \r\r\n",
            "Stay calm, stay safe.\r\r\n",
            "\r\r\n",
            "#COVID19france #COVID_19 #COVID19 #coronavirus #confinement #Confinementotal #ConfinementGeneral https://t.co/zrlG0Z520j\n",
            "\n",
            "Tweet 5: Me, ready to go at supermarket during the #COVID19 outbreak.\r\r\n",
            "\r\r\n",
            "Not because I'm paranoid, but because my food stock is litteraly empty. The #coronavirus is a serious thing, but please, don't panic. It causes shortage...\r\r\n",
            "\r\r\n",
            "#CoronavirusFrance #restezchezvous #StayAtHome #confinement https://t.co/usmuaLq72n\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n Sample of Noisy Tweets (First 5):\")\n",
        "for i, tweet in df['OriginalTweet'].head(5).items():\n",
        "    print(f\"\\nTweet {i+1}: {tweet}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbuhOfDVWSj0"
      },
      "source": [
        "Looking at these noisy tweet examples reveals typical challenges when working with real-world social media text data.Tweets often contain user mentions (e.g., @MeNyrbie), URLs, and hashtags that introduce noise and are usually not useful for sentiment or topic analysis. Removing or replacing these elements helps clean the data without losing meaning.\n",
        "\n",
        "Many tweets include informal language and unstructured sentences, making tokenization, stopword removal, and normalization essential to standardize the text for NLP tasks.\n",
        "\n",
        "Cleaning practices such as removing URLs, converting text to lowercase, eliminating punctuation, and filtering out non-informative tokens are well-documented as foundational steps in preparing tweets for further analysis or modeling.\n",
        "\n",
        "By reviewing and understanding these noisy tweet examples, I base my preprocessing approach on proven text cleaning techniques, ensuring that the data is transformed into a format that models can better understand and learn from."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugc5XspaUty4"
      },
      "source": [
        "# Lowercasing + URL, mention, hashtag, punctuation, \"RT\", and whitespace cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmEfmuYCUg35"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)              # Remove URLs\n",
        "    text = re.sub(r'@\\w+|#\\w+', '', text)                   # Remove mentions and hashtags\n",
        "    text = re.sub(r'\\brt\\b', '', text)                      # Remove 'RT'\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)              # Remove punctuation/special characters\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()                # Remove extra whitespace\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFINV2T0Uzvu"
      },
      "outputs": [],
      "source": [
        "df['Clean_Tweet'] = df['OriginalTweet'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJ81xBzqWk1f"
      },
      "source": [
        "From my what i understand in our lesson, I designed this clean_text function after reading again the lecture note and effective preprocessing techniques for cleaning noisy social media text:\n",
        "\n",
        "- I convert the text to lowercase to standardize the input, so that words are treated consistently during analysis.\n",
        "\n",
        "- I remove URLs using the regex pattern r'http\\S+|www\\S+', which targets common URL patterns encountered in tweets, ensuring these links don't interfere with the text data.\n",
        "\n",
        "- I strip out mentions and hashtags using r'@\\w+|#\\w+' because usernames and hashtags usually add noise if the goal is to analyze the core textual content.\n",
        "\n",
        "- I remove isolated retweet markers \"RT\" with r'\\brt\\b' since these are metadata rather than meaningful content.\n",
        "\n",
        "- Removing punctuation and special characters with r'[^a-zA-Z0-9\\s]' simplifies the text to alphanumeric tokens and whitespace, which many NLP models handle better.\n",
        "\n",
        "- Finally, I normalize extra whitespace to single spaces and trim the text to keep it clean and consistent.\n",
        "\n",
        "This sequence of regex operations is a well-established approach widely documented in text mining and Twitter NLP preprocessing guides. I based this function on such best practices to make sure the tweets I analyze are cleaner and more uniform for downstream tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjuZrT4EU-mH"
      },
      "source": [
        "# Remove Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPkLeIWiVAKB"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    words = text.split()\n",
        "    filtered = [word for word in words if word not in stop_words]\n",
        "    return ' '.join(filtered)\n",
        "\n",
        "    df['NoStop_Tweet'] = df['Clean_Tweet'].apply(remove_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zvHo8emW6eM"
      },
      "source": [
        "I implemented the stopwords removal step after researching established text preprocessing techniques.\n",
        "\n",
        "- I define stop_words as a set of common English stopwords from NLTK because sets allow for efficient membership testing during filtering.\n",
        "\n",
        "- In the remove_stopwords function, I split the input text into individual words, then use a list comprehension to exclude any words found in the stop_words set.\n",
        "\n",
        "- Finally, I join the filtered words back into a string for easy downstream use.\n",
        "\n",
        "This method aligns with recommended practices in NLP preprocessing to reduce noise by removing frequently occurring but semantically weak words (like \"the\", \"is\", \"and\") that typically do not add useful information to models.\n",
        "\n",
        "Applying this to the cleaned tweet text ensures that my input data is more focused and relevant for tasks like sentiment analysis or classification.From my point of view, I implemented stopwords removal by first loading a standard set of English stopwords from NLTK, as this is a widely used and efficient resource for filtering common but uninformative words.\n",
        "\n",
        "My `remove_stopwords` function splits the input text into words, filters out any that are found in the stopword set, and then rejoins the filtered words into a clean string. This approach is a standard technique I researched to reduce noise and improve the quality of text data for NLP tasks by focusing on meaningful content rather than frequent filler words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Xd7nuzlVH5f"
      },
      "source": [
        "\n",
        "\n",
        "# Result of removing stopwwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aO37vp6oVCrZ",
        "outputId": "1ba77157-7f47-4c10-9938-0da5be723d09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample After Stopword Removal (First 5):\n",
            "\n",
            "Before: and and\n",
            "After : \n",
            "\n",
            "Before: advice talk to your neighbours family to exchange phone numbers create contact list with phone numbers of neighbours schools employer chemist gp set up online shopping accounts if poss adequate supplies of regular meds but not over order\n",
            "After : advice talk neighbours family exchange phone numbers create contact list phone numbers neighbours schools employer chemist gp set online shopping accounts poss adequate supplies regular meds order\n",
            "\n",
            "Before: coronavirus australia woolworths to give elderly disabled dedicated shopping hours amid covid19 outbreak\n",
            "After : coronavirus australia woolworths give elderly disabled dedicated shopping hours amid covid19 outbreak\n",
            "\n",
            "Before: my food stock is not the only one which is empty please dont panic there will be enough food for everyone if you do not take more than you need stay calm stay safe\n",
            "After : food stock one empty please dont panic enough food everyone take need stay calm stay safe\n",
            "\n",
            "Before: me ready to go at supermarket during the outbreak not because im paranoid but because my food stock is litteraly empty the is a serious thing but please dont panic it causes shortage\n",
            "After : ready go supermarket outbreak im paranoid food stock litteraly empty serious thing please dont panic causes shortage\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nSample After Stopword Removal (First 5):\")\n",
        "for i, row in df[['Clean_Tweet', 'NoStop_Tweet']].head(5).iterrows():\n",
        "    print(f\"\\nBefore: {row['Clean_Tweet']}\\nAfter : {row['NoStop_Tweet']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jWAJHkHXFhO"
      },
      "source": [
        "Printing the original cleaned tweet alongside its version after stopword removal provides clear evidence of the effect of this cleaning step.\n",
        "\n",
        "- The \"Before\" text shows the tweet after initial cleaning lowercased with links and mentions removed.\n",
        "\n",
        "- The \"After\" text further removes common English stopwords, making the tweet more concise by focusing on meaningful words.\n",
        "\n",
        "- This comparison helps me verify that the stopword removal function is working correctly, aligning with best practices to reduce noise and enhance the quality of text data for sentiment analysis or other NLP tasks.\n",
        "\n",
        "Direct side-by-side comparisons like this are widely used in text preprocessing workflows to validate intermediate steps and ensure data quality improvements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNWXuOvHVT9f"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dD-qvq3ZVQpU",
        "outputId": "93972822-2532-41db-d3c1-32c643345aea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample After Tokenization (First 5):\n",
            "\n",
            "Input: \n",
            "Tokens: []\n",
            "\n",
            "Input: advice talk neighbours family exchange phone numbers create contact list phone numbers neighbours schools employer chemist gp set online shopping accounts poss adequate supplies regular meds order\n",
            "Tokens: ['advice', 'talk', 'neighbours', 'family', 'exchange', 'phone', 'numbers', 'create', 'contact', 'list', 'phone', 'numbers', 'neighbours', 'schools', 'employer', 'chemist', 'gp', 'set', 'online', 'shopping', 'accounts', 'poss', 'adequate', 'supplies', 'regular', 'meds', 'order']\n",
            "\n",
            "Input: coronavirus australia woolworths give elderly disabled dedicated shopping hours amid covid19 outbreak\n",
            "Tokens: ['coronavirus', 'australia', 'woolworths', 'give', 'elderly', 'disabled', 'dedicated', 'shopping', 'hours', 'amid', 'covid19', 'outbreak']\n",
            "\n",
            "Input: food stock one empty please dont panic enough food everyone take need stay calm stay safe\n",
            "Tokens: ['food', 'stock', 'one', 'empty', 'please', 'dont', 'panic', 'enough', 'food', 'everyone', 'take', 'need', 'stay', 'calm', 'stay', 'safe']\n",
            "\n",
            "Input: ready go supermarket outbreak im paranoid food stock litteraly empty serious thing please dont panic causes shortage\n",
            "Tokens: ['ready', 'go', 'supermarket', 'outbreak', 'im', 'paranoid', 'food', 'stock', 'litteraly', 'empty', 'serious', 'thing', 'please', 'dont', 'panic', 'causes', 'shortage']\n"
          ]
        }
      ],
      "source": [
        "df['Tokenized_Tweet'] = df['NoStop_Tweet'].apply(lambda x: x.split())\n",
        "\n",
        "print(\"\\nSample After Tokenization (First 5):\")\n",
        "for i, row in df[['NoStop_Tweet', 'Tokenized_Tweet']].head(5).iterrows():\n",
        "    print(f\"\\nInput: {row['NoStop_Tweet']}\\nTokens: {row['Tokenized_Tweet']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I applied tokenization by splitting each cleaned tweet into individual words using the Python string `.split()` method based on whitespace, which I reading the lecture note that the researcher provide to us  as a straightforward and effective tokenization technique for preprocessed text.\n",
        "\n",
        "- This method transforms the cleaned, stopword-removed tweet string into a list of tokens (words), which is a critical step for most NLP tasks such as feature extraction or modeling.\n",
        "\n",
        "- The simplicity and efficiency of `.split()` make it suitable when the text has already been cleaned and normalized since it treats consecutive whitespace as delimiters.\n",
        "\n",
        "- Printing both the input text and its tokenized form helps me verify that words are correctly separated and ready for further analysis or vectorization.\n",
        "\n",
        "This approach follows best practices for preparing text data and aligns with common NLP workflows documented in multiple tutorials and guides."
      ],
      "metadata": {
        "id": "ZJ1-EI76t0xb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results of cleaned data"
      ],
      "metadata": {
        "id": "O1iK3kSbs2Dc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3QnzjsQU3OM",
        "outputId": "8c258300-fc65-4ca0-a61e-b1a8acf0b497"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample After Cleaning (First 5):\n",
            "\n",
            "Original: @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/iFz9FAn2Pa and https://t.co/xX6ghGFzCC and https://t.co/I2NlzdxNo8\n",
            "Cleaned : and and\n",
            "\n",
            "Original: advice Talk to your neighbours family to exchange phone numbers create contact list with phone numbers of neighbours schools employer chemist GP set up online shopping accounts if poss adequate supplies of regular meds but not over order\n",
            "Cleaned : advice talk to your neighbours family to exchange phone numbers create contact list with phone numbers of neighbours schools employer chemist gp set up online shopping accounts if poss adequate supplies of regular meds but not over order\n",
            "\n",
            "Original: Coronavirus Australia: Woolworths to give elderly, disabled dedicated shopping hours amid COVID-19 outbreak https://t.co/bInCA9Vp8P\n",
            "Cleaned : coronavirus australia woolworths to give elderly disabled dedicated shopping hours amid covid19 outbreak\n",
            "\n",
            "Original: My food stock is not the only one which is empty...\r\r\n",
            "\r\r\n",
            "PLEASE, don't panic, THERE WILL BE ENOUGH FOOD FOR EVERYONE if you do not take more than you need. \r\r\n",
            "Stay calm, stay safe.\r\r\n",
            "\r\r\n",
            "#COVID19france #COVID_19 #COVID19 #coronavirus #confinement #Confinementotal #ConfinementGeneral https://t.co/zrlG0Z520j\n",
            "Cleaned : my food stock is not the only one which is empty please dont panic there will be enough food for everyone if you do not take more than you need stay calm stay safe\n",
            "\n",
            "Original: Me, ready to go at supermarket during the #COVID19 outbreak.\r\r\n",
            "\r\r\n",
            "Not because I'm paranoid, but because my food stock is litteraly empty. The #coronavirus is a serious thing, but please, don't panic. It causes shortage...\r\r\n",
            "\r\r\n",
            "#CoronavirusFrance #restezchezvous #StayAtHome #confinement https://t.co/usmuaLq72n\n",
            "Cleaned : me ready to go at supermarket during the outbreak not because im paranoid but because my food stock is litteraly empty the is a serious thing but please dont panic it causes shortage\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nSample After Cleaning (First 5):\")\n",
        "for i, row in df[['OriginalTweet', 'Clean_Tweet']].head(5).iterrows():\n",
        "    print(f\"\\nOriginal: {row['OriginalTweet']}\\nCleaned : {row['Clean_Tweet']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAcMqRffXQps"
      },
      "source": [
        "This line `tokenizes `each cleaned tweet by splitting it into individual words, or tokens. It applies a `lambda` function to the Clean_Tweet column, using Python's built-in` split()` method to break each string into a list of words based on spaces. The resulting tokens are stored in a new column called `Tokenized_Tweet`. This step is essential for most natural language processing tasks, as it converts unstructured text into a structured format that algorithms can process more easily.\n",
        "\n",
        " I used a loop to go through the first 5 tweets from the` NoStop_Tweet` column (which already had stopwords removed), and then printed each tweet along with its corresponding list of tokens from the `Tokenized_Tweet` column. The printout is labeled clearly‚Äî\"Input\" shows the sentence before tokenization, and \"Tokens\" shows how the words were split into a list format. This helps me confirm that the text has been properly broken down into parts that can be used later for analysis, like building models or counting word frequency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p4xl-GCVdAP"
      },
      "source": [
        "# Exporting cleaned tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RMtV8ANVaIb",
        "outputId": "859dd69a-05a0-466d-9af1-0cc3450bc28f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Exported cleaned tweets to 'cleaned_tweets.csv'\n"
          ]
        }
      ],
      "source": [
        "df[['OriginalTweet', 'Clean_Tweet', 'NoStop_Tweet', 'Tokenized_Tweet']].to_csv('cleaned_tweets.csv', index=False)\n",
        "print(\"\\nExported cleaned tweets to 'cleaned_tweets.csv'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykOYx0GAXgtr"
      },
      "source": [
        "\n",
        "This line exports the selected columns  `OriginalTweet, Clean_Tweet,` and `Tokenized_Tweet ` to a new CSV file named cleaned_tweets.csv. By specifying `index=False`, it ensures that the row index is not included in the output file. This step is useful for saving the cleaned and tokenized data so it can be reused later for analysis, modeling, or sharing, without having to repeat the preprocessing steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HqKhEkbWeEA"
      },
      "source": [
        "# Existing Noise\n",
        "Typical noise in this dataset:\n",
        "\n",
        "- URLs (http://...)\n",
        "\n",
        "- Hashtags (#covid)\n",
        "\n",
        "- Mentions (@user)\n",
        "\n",
        "- Emojis / Non-ASCII characters (üò∑)\n",
        "\n",
        "- HTML Entities (&amp;)\n",
        "\n",
        "- Punctuation (?!,:)\n",
        "\n",
        "- Mixed casing (COVID, Covid, covid)\n",
        "\n",
        "New Noise (Not discussed in class):\n",
        "- RT or ‚Äúretweet‚Äù headers, which do not add value in context.\n",
        "\n",
        "# Data Format\n",
        "\n",
        "- Format: CSV\n",
        "\n",
        "- Relevant Text Column: Original_Tweet\n",
        "\n",
        "# Language\n",
        "- Language: English"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing Own Dataset"
      ],
      "metadata": {
        "id": "H6TQOpkPfl4n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing Libraries**"
      ],
      "metadata": {
        "id": "OA07RL0ihSuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string"
      ],
      "metadata": {
        "id": "6K9lfxYUhXl0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code imports three important Python libraries that are essential for data manipulation and text processing, which suggests that the upcoming code will likely involve working with data and cleaning or analyzing text.\n",
        "\n",
        "From my understanding:\n",
        "\n",
        "- pandas as pd: I imported pandas, a powerful library used to work with structured data in the form of dataframes. This library is crucial for data manipulation, reading files like CSV or Excel, and performing complex data analysis efficiently.\n",
        "\n",
        "- re: This is the regular expressions library that I included to help with pattern matching and searching within text. It enables me to find, split, or replace specific patterns in strings, which is very useful for cleaning or extracting information from textual data.\n",
        "\n",
        "- string: This module provides a collection of string constants and utility functions that I utilize to handle common string operations. For example, it gives access to pre-defined lists like all punctuation marks, which helps in removing or processing punctuation from text.\n",
        "\n",
        "Putting these together, I believe the code is setting up an environment where I can load or handle data, then perform text cleaning or extraction processes by leveraging regular expressions and string utilities. This foundational setup is typical in data preprocessing workflows, especially when preparing textual data for analysis or machine learning tasks."
      ],
      "metadata": {
        "id": "gZDJZVKvjdaU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading the Dataset**"
      ],
      "metadata": {
        "id": "x6WSNwIshZUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Step 1: Load your dataset with proper encoding\n",
        "# ==============================\n",
        "df = pd.read_csv(\"Almazan_ManualData(Sheet1).csv\", encoding=\"latin1\")\n",
        "# Alternative encodings if needed: encoding=\"ISO-8859-1\" or encoding=\"cp1252\""
      ],
      "metadata": {
        "id": "i-0hQAKYhbce"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part of the code is focused on loading the dataset correctly into memory. Specifically, I use the pd.read_csv function from pandas to read a CSV file named \"Almazan_ManualData(Sheet1).csv\" into a dataframe called df.\n",
        "\n",
        "I pay special attention to the encoding parameter by setting it to **\"latin1\"**. This is important because different CSV files might use different character encodings, and specifying the right encoding ensures that special characters, accents, or non-English text are correctly interpreted without causing errors or data corruption.\n",
        "\n",
        "I also note that if **\"latin1\"** does not work properly, I have alternative encodings like **\"ISO-8859-1\" or \"cp1252\"** that I can try. This shows that I am aware of potential encoding issues and am prepared to handle them to ensure smooth and accurate data loading, which is a critical first step in any data processing or analysis task."
      ],
      "metadata": {
        "id": "0IoY13F4jtCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define cleaning function**\n"
      ],
      "metadata": {
        "id": "ptpFjSxoh8Am"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    if pd.isnull(text):\n",
        "        return \"\"\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Remove URLs\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text"
      ],
      "metadata": {
        "id": "u2Jxlx1Sh4Em"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In my understanding, this clean_text function is designed to preprocess and clean text data in a systematic way, preparing it for further analysis or modeling.\n",
        "\n",
        "Here is how I interpret its functionality step-by-step:\n",
        "\n",
        "- First, I check if the input text is null or missing by using pd.isnull(text). If it is, I return an empty string to avoid errors in processing and ensure consistent handling of missing data.\n",
        "\n",
        "- Next, I convert the entire text to lowercase with text.lower(). This normalization step helps in treating words like \"Apple\" and \"apple\" as the same, which is important for text analysis.\n",
        "\n",
        "- Then, I remove URLs from the text using a regular expression (re.sub). By targeting patterns starting with http, www, or https, I ensure that any web links are stripped out, which are often noise in textual data.\n",
        "\n",
        "- After that, I remove all punctuation using the translate function combined with string.punctuation. This step cleans the text further by discarding symbols like commas, periods, and other non-alphanumeric characters that might not be helpful in some analyses.\n",
        "\n",
        "- Finally, I remove any extra whitespace by replacing multiple spaces with a single space and trimming leading/trailing spaces using re.sub with strip(). This guarantees a neat and consistent format for the text."
      ],
      "metadata": {
        "id": "WJbzO9hakNxT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Apply cleaning**"
      ],
      "metadata": {
        "id": "-2YeAEKgiasJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"CleanedEmail\"] = df[\"EmailContent\"].apply(clean_text)"
      ],
      "metadata": {
        "id": "2ybRWuODiZBc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line of code applies the clean_text function I defined earlier to the \"EmailContent\" column in the dataframe df. By using the .apply() method, I process each individual email text entry, cleaning it according to the steps I researched and implemented.\n",
        "\n",
        "The cleaned text is then stored in a new column called \"CleanedEmail.\" This approach allows me to preserve the original email content while also creating a separate column with standardized, cleaned text. This is crucial for any subsequent analysis, such as text mining or machine learning, where consistent and clean input data significantly improves results."
      ],
      "metadata": {
        "id": "Ptz6yIBpkcBl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Showing Original vs Cleaned"
      ],
      "metadata": {
        "id": "ZXfpCcf0iffJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(df)):\n",
        "    print(f\"Original: {df.loc[i, 'EmailContent']}\")\n",
        "    print(f\"Cleaned : {df.loc[i, 'CleanedEmail']}\")\n",
        "    print(\"-\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsADxeWsidR3",
        "outputId": "e8233d15-42af-4e98-de66-9d6e27246cbb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: Free COVID-19 masks available! Click here to claim your package.\n",
            "Cleaned : free covid19 masks available click here to claim your package\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Reminder: Stay home and practice social distancing to prevent virus spread.\n",
            "Cleaned : reminder stay home and practice social distancing to prevent virus spread\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Urgent! Buy our miracle cure for coronavirus, limited stock available.\n",
            "Cleaned : urgent buy our miracle cure for coronavirus limited stock available\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Your employer advises all staff to work from home starting tomorrow.\n",
            "Cleaned : your employer advises all staff to work from home starting tomorrow\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Congratulations! You have won free groceries during the COVID-19 lockdown.\n",
            "Cleaned : congratulations you have won free groceries during the covid19 lockdown\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Local government announces stricter travel restrictions effective tonight.\n",
            "Cleaned : local government announces stricter travel restrictions effective tonight\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Claim your COVID-19 relief fund by sending your bank details today!\n",
            "Cleaned : claim your covid19 relief fund by sending your bank details today\n",
            "--------------------------------------------------------------------------------\n",
            "Original: School closure update: classes will continue online starting next week.\n",
            "Cleaned : school closure update classes will continue online starting next week\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Buy COVID insurance now! Protect your family instantly.\n",
            "Cleaned : buy covid insurance now protect your family instantly\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Daily health tip: wash your hands for 20 seconds to reduce infection risk.\n",
            "Cleaned : daily health tip wash your hands for 20 seconds to reduce infection risk\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Free sanitizer giveaway √¢¬Ä¬ì just cover shipping fees of $9.99!\n",
            "Cleaned : free sanitizer giveaway √¢¬Ä¬ì just cover shipping fees of 999\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Community update: grocery stores will open 1 hour early for seniors.\n",
            "Cleaned : community update grocery stores will open 1 hour early for seniors\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Get your COVID-19 results instantly by paying online here.\n",
            "Cleaned : get your covid19 results instantly by paying online here\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Public transport will run on limited schedules during the lockdown.\n",
            "Cleaned : public transport will run on limited schedules during the lockdown\n",
            "--------------------------------------------------------------------------------\n",
            "Original: You are selected for a free COVID testing kit! Click to register now.\n",
            "Cleaned : you are selected for a free covid testing kit click to register now\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Reminder: wear a mask at all times in public places.\n",
            "Cleaned : reminder wear a mask at all times in public places\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Invest in coronavirus vaccine stocks now and double your money.\n",
            "Cleaned : invest in coronavirus vaccine stocks now and double your money\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Meeting rescheduled to online format due to COVID-19 restrictions.\n",
            "Cleaned : meeting rescheduled to online format due to covid19 restrictions\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Limited time! Buy disinfectant sprays at 80% off today only.\n",
            "Cleaned : limited time buy disinfectant sprays at 80 off today only\n",
            "--------------------------------------------------------------------------------\n",
            "Original: COVID-19 hotline available 24/7 for health concerns.\n",
            "Cleaned : covid19 hotline available 247 for health concerns\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Urgent! Your package is contaminated, pay fee to disinfect it now.\n",
            "Cleaned : urgent your package is contaminated pay fee to disinfect it now\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Government launches financial aid program for affected families.\n",
            "Cleaned : government launches financial aid program for affected families\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Free vitamins to boost your immunity against COVID, claim today!\n",
            "Cleaned : free vitamins to boost your immunity against covid claim today\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Office will remain closed until further notice due to safety guidelines.\n",
            "Cleaned : office will remain closed until further notice due to safety guidelines\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Donate now to fight coronavirus and get free rewards in return.\n",
            "Cleaned : donate now to fight coronavirus and get free rewards in return\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Online classes will start next week for all students.\n",
            "Cleaned : online classes will start next week for all students\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Claim government relief by clicking this link and entering your SSN.\n",
            "Cleaned : claim government relief by clicking this link and entering your ssn\n",
            "--------------------------------------------------------------------------------\n",
            "Original: COVID-19 update: vaccination center timings revised.\n",
            "Cleaned : covid19 update vaccination center timings revised\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Double your income during lockdown with our online scheme!\n",
            "Cleaned : double your income during lockdown with our online scheme\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Hospitals request volunteers to donate blood urgently.\n",
            "Cleaned : hospitals request volunteers to donate blood urgently\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Get rich working from home during COVID crisis. Sign up free!\n",
            "Cleaned : get rich working from home during covid crisis sign up free\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Local health department announces free testing locations.\n",
            "Cleaned : local health department announces free testing locations\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Miracle herbal cure for COVID √¢¬Ä¬ì proven effective!\n",
            "Cleaned : miracle herbal cure for covid √¢¬Ä¬ì proven effective\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Grocery delivery service available for all quarantined residents.\n",
            "Cleaned : grocery delivery service available for all quarantined residents\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Win a brand-new car by participating in our COVID survey.\n",
            "Cleaned : win a brandnew car by participating in our covid survey\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Reminder: avoid crowded places to prevent virus spread.\n",
            "Cleaned : reminder avoid crowded places to prevent virus spread\n",
            "--------------------------------------------------------------------------------\n",
            "Original: COVID-19 special lottery tickets now on sale!\n",
            "Cleaned : covid19 special lottery tickets now on sale\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Free online counseling available for those affected by pandemic stress.\n",
            "Cleaned : free online counseling available for those affected by pandemic stress\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Earn $500 daily with our COVID relief investment plan.\n",
            "Cleaned : earn 500 daily with our covid relief investment plan\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Workplace safety rules updated: masks mandatory at all times.\n",
            "Cleaned : workplace safety rules updated masks mandatory at all times\n",
            "--------------------------------------------------------------------------------\n",
            "Original: You have been chosen for free grocery vouchers during lockdown.\n",
            "Cleaned : you have been chosen for free grocery vouchers during lockdown\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Local schools to remain closed for another two weeks.\n",
            "Cleaned : local schools to remain closed for another two weeks\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Limited stock! Buy COVID vaccines privately now.\n",
            "Cleaned : limited stock buy covid vaccines privately now\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Supermarket introduces senior-only shopping hours in the morning.\n",
            "Cleaned : supermarket introduces senioronly shopping hours in the morning\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Special COVID charity raffle √¢¬Ä¬ì donate and win cash prizes!\n",
            "Cleaned : special covid charity raffle √¢¬Ä¬ì donate and win cash prizes\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Hospital announces vaccination slots now open for registration.\n",
            "Cleaned : hospital announces vaccination slots now open for registration\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Claim instant cash relief by sharing your personal details here.\n",
            "Cleaned : claim instant cash relief by sharing your personal details here\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Community center offers free masks for residents.\n",
            "Cleaned : community center offers free masks for residents\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Quick wealth opportunity during COVID √¢¬Ä¬ì enroll today!\n",
            "Cleaned : quick wealth opportunity during covid √¢¬Ä¬ì enroll today\n",
            "--------------------------------------------------------------------------------\n",
            "Original: City government distributes relief goods to households this weekend.\n",
            "Cleaned : city government distributes relief goods to households this weekend\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Free Netflix subscription during lockdown √¢¬Ä¬ì register here!\n",
            "Cleaned : free netflix subscription during lockdown √¢¬Ä¬ì register here\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Vaccination reminders: please bring ID to your appointment.\n",
            "Cleaned : vaccination reminders please bring id to your appointment\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Win millions by joining our COVID survival contest!\n",
            "Cleaned : win millions by joining our covid survival contest\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Health advisory: symptoms include fever, cough, and shortness of breath.\n",
            "Cleaned : health advisory symptoms include fever cough and shortness of breath\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Lockdown survival kit available now √¢¬Ä¬ì order yours today!\n",
            "Cleaned : lockdown survival kit available now √¢¬Ä¬ì order yours today\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Government extends travel ban for another week.\n",
            "Cleaned : government extends travel ban for another week\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Free crypto giveaway during COVID! Claim your coins now.\n",
            "Cleaned : free crypto giveaway during covid claim your coins now\n",
            "--------------------------------------------------------------------------------\n",
            "Original: New guidelines: maintain 2 meters distance in all public places.\n",
            "Cleaned : new guidelines maintain 2 meters distance in all public places\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Exclusive offer: buy COVID antibody kits online today.\n",
            "Cleaned : exclusive offer buy covid antibody kits online today\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Public transport will remain operational with safety protocols in place.\n",
            "Cleaned : public transport will remain operational with safety protocols in place\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this loop iterates through every row in the dataframe df and prints out the original email content alongside its cleaned version generated by the clean_text function.\n",
        "\n",
        "By doing this, I can visually compare the raw text to its processed counterpart line by line. This helps me verify that the cleaning steps (lowercasing, URL removal, punctuation stripping, and whitespace normalization) are working correctly and consistently.\n",
        "\n",
        "The sample outputs I reviewed demonstrate that:\n",
        "\n",
        "- URLs are removed accurately.\n",
        "\n",
        "- Punctuation is stripped except for certain encoded characters which persist (e.g., \"√¢¬Ä¬ì\").\n",
        "\n",
        "- Text is uniformly lowercased.\n",
        "\n",
        "- Extra spaces are removed.\n",
        "\n",
        "- The core meaning and readability of the text remain intact after cleaning.\n",
        "\n",
        "This side-by-side comparison, which I researched as a good data validation practice, gives me confidence that the preprocessing pipeline is effective and ready for further text analysis or modeling tasks."
      ],
      "metadata": {
        "id": "gEB91hL8kwE6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save to Excel**"
      ],
      "metadata": {
        "id": "iEFOxMI1ikbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_excel(\"Almazan_ManualData_Cleaned.xlsx\", index=False)\n",
        "\n",
        "print(\"‚úÖ Cleaning complete! File saved as 'Almazan_ManualData_Cleaned.xlsx'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xroXHjsijOG",
        "outputId": "ea8aa39e-8bb8-43b8-8e9e-bed653357caf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Cleaning complete! File saved as 'Almazan_ManualData_Cleaned.xlsx'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code line df.to_excel(\"Almazan_ManualData_Cleaned.xlsx\", index=False) uses pandas' built-in to_excel method to export the DataFrame df into an Excel file named \"Almazan_ManualData_Cleaned.xlsx\". The parameter index=False ensures that the row indices are not included in the saved file, which is commonly done to keep the Excel sheet clean and focused on the actual data columns.\n",
        "\n",
        "This method is a standard and efficient way in pandas to save processed data for sharing, archiving, or further use in Excel-compatible applications. It allows the cleaned dataset to be stored persistently after all preprocessing steps are complete.\n",
        "\n",
        "\n",
        "From my understanding on text preprocessing in our lesson best practices in natural language processing (NLP), the steps implemented in the `clean_text `function are strongly validated by widely accepted techniques.\n",
        "\n",
        "Key points supporting this include:\n",
        "\n",
        "- Data cleaning and normalization are foundational in NLP to transform raw text into a consistent and usable format. Removing unwanted elements like URLs and punctuation helps reduce noise that could skew results.\n",
        "\n",
        "- Lowercasing text standardizes the input so that words differing only by case are treated uniformly, which enhances analytical accuracy.\n",
        "\n",
        "- Removing punctuation is a common preprocessing step to focus on meaningful words while discarding symbols that don't contribute significantly to text semantics, though some special punctuation may sometimes be handled with care depending on context.\n",
        "\n",
        "- Eliminating URLs is important because they often introduce irrelevant tokens that do not provide useful information for most text analysis tasks.\n",
        "\n",
        "- Removing extra spaces and trimming whitespace ensures tidy data and prevents formatting issues during tokenization or parsing.\n",
        "\n",
        "- Handling missing or null text inputs by returning an empty string is aligned with best practices for gracefully managing missing data in textual datasets.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7tZUHAA-lKk3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWGFA_qY40fc"
      },
      "source": [
        "\n",
        "\n",
        "# NLP Application Text Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuFk1rpmQEIP"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.training import Example\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8B9J41AhnFBS"
      },
      "source": [
        "When I dug into the code and its imports, I realized how thoughtfully chosen these libraries are for natural language processing and model evaluation.\n",
        "\n",
        "First, spaCy stood out as a really powerful library for NLP. I learned that it‚Äôs designed not just for basic text processing but also supports building and training custom language models efficiently. The Example class was something new to me  it‚Äôs a structured way to create annotated training samples so the model can learn from real examples, which is crucial for improving accuracy.\n",
        "\n",
        "Then, there‚Äôs pandas, which I‚Äôve come to appreciate as the go-to tool for managing datasets. Its DataFrame structure makes it straightforward to clean, explore, and manipulate data, which is critical before feeding it into any NLP pipeline.\n",
        "\n",
        "I also explored scikit-learn and understood why it‚Äôs included. The function `train_test_split` is vital because it helps me split the data properly, making sure that the model is tested on unseen data  avoiding overfitting and giving me a true sense of how well it performs. The metrics I chose to import **accuracy, precision, recall, and F1-score**  each offer a different lens on model quality, helping me to assess not just if the model gets things right, but also how well it balances false positives and false negatives.\n",
        "\n",
        "Collectively, these libraries provide a solid foundation. From data handling to NLP modeling and thorough evaluation, I can confidently say these imports reflect a deep understanding of the workflow needed for successful NLP projects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYF34jX_QFwX"
      },
      "outputs": [],
      "source": [
        "# =====================\n",
        "# Load Excel file\n",
        "# =====================\n",
        "# Define the path of the Excel file containing our manually prepared dataset\n",
        "file_path = \"Almazan_ManualData_Cleaned.xlsx\"\n",
        "\n",
        "# Read the Excel file into a pandas DataFrame for easier handling and analysis\n",
        "df = pd.read_excel(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k00ZRmDnc16"
      },
      "source": [
        "I found out that the function automatically reads the first sheet in the Excel file unless otherwise specified, making it simple to start working with data quickly. Having the data in a DataFrame allows me to easily explore, clean, and manipulate the dataset as needed for my NLP tasks.\n",
        "\n",
        "This approach ensures that my manually prepared dataset stored in \"Almazan_ManualData_Cleaned.xlsx\" is loaded efficiently and ready for further processing with minimal hassle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YHMi7jmSaI2"
      },
      "outputs": [],
      "source": [
        "# Extract features and labels\n",
        "\n",
        "# Select the \"CleanedEmail\" column as features (input text data)\n",
        "emails = df[\"CleanedEmail\"].astype(str)    # Ensure values are strings\n",
        "# Select the \"Label\" column as targets (output categories: SPAM/HAM)\n",
        "labels = df[\"Label\"].astype(str)           # Ensure values are strings\n",
        "\n",
        "# Divide dataset into training and testing sets\n",
        "# test_size=0.2 ‚Üí 20% test data, 80% training data\n",
        "# random_state=42 ‚Üí ensures reproducibility of the split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    emails, labels, test_size=0.2, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDUWJv6xn-Ax"
      },
      "source": [
        "When I examined this part of the code, I learned that selecting the right features and labels is essential for building effective machine learning models. Here, the \"CleanedEmail\" column contains the processed text data that will serve as input features, so I made sure to convert the values to strings to avoid any type issues. The \"Label\" column holds the target categories, like SPAM or HAM, also converted to strings to maintain consistency.\n",
        "\n",
        "For creating training and testing datasets, I used sklearn's `train_test_split`. Upon reading the website of geeksforgeeks.org this function is invaluable for reliably splitting the data. By specifying test_size=0.2, I reserve 20% of the data for testing to evaluate the model's performance on unseen examples. The random_state=42 parameter ensures that the split is reproducible, so future runs yield the same train-test division. This helps me trust that the model evaluation remains fair and consistent over time."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================\n",
        "# Build SpaCy Text Classifier\n",
        "# =====================\n",
        "\n",
        "# Initialize a blank English NLP pipeline\n",
        "nlpTC = spacy.blank(\"en\")\n",
        "\n",
        "# Add a text classification component (textcat) to the pipeline\n",
        "textcat = nlpTC.add_pipe(\"textcat\")\n",
        "\n",
        "# Define the two categories for classification: SPAM and HAM\n",
        "textcat.add_label(\"SPAM\")\n",
        "textcat.add_label(\"HAM\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjTRKT8EOFp7",
        "outputId": "9a3d5686-5b1e-4dd0-9ab2-5c4642d05196"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In exploring how to build a text classifier with spaCy, I learned that starting with a blank NLP pipeline for the English language lets me customize every component to suit my needs. Instead of loading a pre-built model, creating a blank pipeline offers flexibility, especially for specialized tasks like spam detection.\n",
        "\n",
        "Adding the textcat component introduces a dedicated text categorization module into the pipeline. This component will be trained to classify entire texts  in my case, emails  into predefined categories.\n",
        "\n",
        "Defining the two labels, \"SPAM\" and \"HAM\", sets the foundation for the classifier to distinguish between unwanted and legitimate messages. This setup aligns with typical binary text classification tasks in NLP.\n",
        "\n",
        "I also found spaCy‚Äôs approach both elegant and efficient, letting me build and train a text classification model using a modular pipeline that can be extended as needed. It also integrates well with spaCy‚Äôs training utilities, which should help with creating examples and optimizing the classifier during training."
      ],
      "metadata": {
        "id": "PyoCbF2dOh-o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8B7v2rzQKgZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# =====================\n",
        "# Prepare training data\n",
        "# =====================\n",
        "\n",
        "# Initialize an empty list to hold training examples\n",
        "train_data = []\n",
        "\n",
        "# Loop through each email text and its corresponding label\n",
        "for text, label in zip(X_train, y_train):\n",
        "    # If the label is SPAM, encode it with {\"SPAM\":1, \"HAM\":0}\n",
        "    if label.upper() == \"SPAM\":\n",
        "        train_data.append((text, {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}))\n",
        "    # Otherwise, encode it with {\"SPAM\":0, \"HAM\":1}\n",
        "    else:\n",
        "        train_data.append((text, {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk3eH7wXohtE"
      },
      "source": [
        "As I examined this block, I learned that spaCy expects training examples for text classification to be a list of tuples. Each tuple contains the raw text and a dictionary with a \"cats\" key representing the text labels as a dictionary of categories mapped to binary indicators (1 or 0).\n",
        "\n",
        "In my case, since I‚Äôm working on a binary classification problem distinguishing **\"SPAM\" vs \"HAM\"**, I had to encode the labels exactly as `{\"SPAM\": 1, \"HAM\": 0} `when the email is spam, and vice versa when it is ham. This clear binary encoding is critical because spaCy uses these values internally to calculate loss during training.\n",
        "\n",
        "Converting my training data into this specific format ensures compatibility with spaCy‚Äôs training pipeline, enabling it to correctly interpret each example‚Äôs label to learn the right classification boundaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5jWWn-TQO7m"
      },
      "outputs": [],
      "source": [
        "# =====================\n",
        "# Training\n",
        "# =====================\n",
        "\n",
        "# Initialize the optimizer for the text classifier\n",
        "optimizer = nlpTC.initialize()\n",
        "\n",
        "# Loop through each training example (text + category annotations)\n",
        "for text, annotations in train_data:\n",
        "    # Convert text and its labels into a spaCy Example object\n",
        "    example = Example.from_dict(nlpTC.make_doc(text), annotations)\n",
        "\n",
        "    # Update the model with the new example (perform one training step)\n",
        "    nlpTC.update([example], sgd=optimizer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDnmgJ5oyNN"
      },
      "source": [
        "While studying this training process, I understood that spaCy requires an optimizer to adjust the model‚Äôs weights during training. Initializing the optimizer with `nlpTC.initialize()` sets up the machinery needed for the learning steps.\n",
        "\n",
        "Each training example (a pair of text and category annotations) is converted into a spaCy `Example` object, which standardizes how the training data is fed into the model. This encapsulation is important because it aligns the raw text with its annotations so spaCy can compute the prediction errors accurately.\n",
        "\n",
        "Calling `nlpTC.update()` performs a single training step on each example, applying stochastic gradient descent (SGD) using the optimizer to minimize the loss function. This iterative process gradually improves the classifier‚Äôs ability to predict the correct category by tweaking internal parameters.\n",
        "\n",
        "I also learned that presenting the model with examples one at a time is a simple way to train, although batching examples together can speed up training and improve stability.\n",
        "\n",
        "Overall, this loop mirrors the core mechanics of statistical machine learning models  repeatedly exposing the model to training data, evaluating its predictions, and updating it to reduce errors. Understanding this reinforced the importance of the training loop‚Äôs role in building an effective text classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X85yKzxTRG6w",
        "outputId": "7ed0dddf-4621-4afc-8425-e59ccba2e694"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Now, enter a sample email for classification (or type 'exit' to quit): urgent buy our miracle cure for coronavirus limited stock available\n",
            "The email is classified as: SPAM\n",
            "Now, enter a sample email for classification (or type 'exit' to quit): your employer advises all staff to work from home starting tomorrow\n",
            "The email is classified as: HAM\n",
            "Now, enter a sample email for classification (or type 'exit' to quit): congratulations you have won free groceries during the covid19 lockdown\n",
            "The email is classified as: SPAM\n",
            "Now, enter a sample email for classification (or type 'exit' to quit): local government announces stricter travel restrictions effective tonight\n",
            "The email is classified as: HAM\n",
            "Now, enter a sample email for classification (or type 'exit' to quit): claim your covid19 relief fund by sending your bank details today\n",
            "The email is classified as: SPAM\n",
            "Now, enter a sample email for classification (or type 'exit' to quit): buy covid insurance now protect your family instantly\n",
            "The email is classified as: SPAM\n",
            "Now, enter a sample email for classification (or type 'exit' to quit): exit\n",
            "Evaluation Metrics:\n",
            "Accuracy : 0.8333333333333334\n",
            "Recall   : 0.7142857142857143\n",
            "F1 Score : 0.8333333333333334\n"
          ]
        }
      ],
      "source": [
        "# =====================\n",
        "# User input loop\n",
        "# =====================\n",
        "\n",
        "# Keep asking the user to enter emails until they type 'exit'\n",
        "while True:\n",
        "    user_input = input(\"Now, enter a sample email for classification (or type 'exit' to quit): \")\n",
        "\n",
        "    # Break the loop if the user wants to quit\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "\n",
        "    # Classify the input email using the helper function\n",
        "    classification = classify_email(user_input)\n",
        "    print(f\"The email is classified as: {classification}\")\n",
        "\n",
        "# =====================\n",
        "# Evaluation\n",
        "# =====================\n",
        "\n",
        "# Initialize an empty list to store predictions\n",
        "y_pred = []\n",
        "\n",
        "# Loop through the test set and predict spam/ham for each email\n",
        "for text in X_test:\n",
        "    doc = nlpTC(text)  # Process text with the trained classifier\n",
        "\n",
        "    # Compare scores and choose the higher one\n",
        "    if doc.cats[\"SPAM\"] > doc.cats[\"HAM\"]:\n",
        "        y_pred.append(\"SPAM\")\n",
        "    else:\n",
        "        y_pred.append(\"HAM\")\n",
        "\n",
        "# Print out evaluation results using accuracy, recall, and F1 score\n",
        "print(\"Evaluation Metrics:\")\n",
        "print(\"Accuracy :\", accuracy_score(y_test, y_pred))\n",
        "print(\"Recall   :\", recall_score(y_test, y_pred, pos_label=\"SPAM\"))\n",
        "print(\"F1 Score :\", f1_score(y_test, y_pred, pos_label=\"SPAM\"))\n",
        "\n",
        "# =====================\n",
        "# Function to classify user input emails\n",
        "# =====================\n",
        "\n",
        "# Define a function that classifies a given email as SPAM or HAM\n",
        "def classify_email(email):\n",
        "    doc = nlpTC(email)  # Process input text\n",
        "    spam_score = doc.cats['SPAM']  # Probability score for SPAM\n",
        "    ham_score = doc.cats['HAM']    # Probability score for HAM\n",
        "\n",
        "    # Return the label with the higher score\n",
        "    return \"SPAM\" if spam_score > ham_score else \"HAM\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnrtgVRUpFil"
      },
      "source": [
        "I understood the importance of making the classifier interactive by repeatedly allowing users to enter emails and see real-time classification results. The loop keeps asking for input until the user types 'exit', which makes it user-friendly and practical for testing the model on arbitrary text examples.\n",
        "\n",
        "The classify_email function uses the trained spaCy pipeline to process the input text and returns the label *(\"SPAM\" or \"HAM\")* based on which category has the higher confidence score `(doc.cats)`. This direct comparison between category scores ensures an intuitive and clear classification decision each time.\n",
        "\n",
        "For the evaluation part, I learned that applying the trained model on the test set and collecting predictions allows me to compare these outputs against the known labels `(y_test)`. Using established metrics from scikit-learn like accuracy, recall (with SPAM as the positive label), and F1 score provides a comprehensive view of model performance, especially in detecting SPAM correctly. Understanding that recall measures the ability to identify true positives helped me appreciate why it‚Äôs critical in spam detection where missing SPAM can have serious consequences.\n",
        "\n",
        "In summary, this code lets me interactively validate the model and quantitatively measure its effectiveness on unseen data, combining human-in-the-loop testing with formal evaluation metrics for confidence in its reliability.\n",
        "\n",
        "- Accuracy (~83.3%) tells me that overall, the model correctly classified about 83% of the emails. This gives a general sense of how well the classifier performed across all categories.\n",
        "\n",
        "- Recall (~71.4%) for the SPAM class indicates that the model identified roughly 71% of all actual spam emails correctly. Since recall measures how many true positives are captured out of all real positives, I learned it‚Äôs especially important in spam detection because missing spam emails (false negatives) could be problematic.\n",
        "\n",
        "- F1 Score (~83.3%) balances both precision and recall, so a high F1 suggests that the model not only identifies spam well but also keeps false alarms relatively low. This was reassuring since it means my model strikes a good balance between catching spam and not misclassifying legitimate emails."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7rqaDDe4sh-"
      },
      "source": [
        "# NLP Application Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvwjgPBAMUsI"
      },
      "outputs": [],
      "source": [
        "# =====================\n",
        "# Import required libraries\n",
        "# =====================\n",
        "\n",
        "import spacy  # spaCy: NLP library for text processing and model training\n",
        "from spacy.training import Example  # Example: formats training data for spaCy\n",
        "import random  # Python's built-in module for generating random numbers (useful for shuffling data)\n",
        "\n",
        "import pandas as pd  # pandas: handles structured data in tabular form\n",
        "\n",
        "# Train-test split from scikit-learn\n",
        "from sklearn.model_selection import train_test_split  # splits dataset into training and testing sets\n",
        "\n",
        "# Evaluation metrics from scikit-learn\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "# accuracy_score   -> overall correctness of predictions\n",
        "# precision_score  -> proportion of predicted positives that are correct\n",
        "# recall_score     -> proportion of actual positives correctly identified\n",
        "# f1_score         -> balance between precision and recall\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oq_UE3c8p_BQ"
      },
      "source": [
        "spaCy: I learned that spaCy is a comprehensive natural language processing library that not only helps process and analyze text data but also supports training custom language models. The Example class is particularly useful because it formats the training data in a way that spaCy can efficiently consume for training.\n",
        "\n",
        "random: I included Python‚Äôs built-in `random` module to aid operations like shuffling data, which is often important to ensure that the model training sees data in varied orders to prevent overfitting.\n",
        "\n",
        "pandas: From my exploration, pandas is critical for handling datasets, especially when structured tabular data is involved. I use it to load and manipulate datasets easily, which streamlines the preprocessing before feeding data to the model.\n",
        "\n",
        "scikit-learn: I found out that this library provides robust utilities for splitting datasets and for evaluation. The `train_test_split` function lets me divide my dataset into training and testing subsets. This separation is crucial because the training set helps the model learn patterns, while the testing set, unseen during training, evaluates how well the model generalizes."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================\n",
        "# Load Excel file\n",
        "# =====================\n",
        "\n",
        "# File path to the manually cleaned dataset\n",
        "file_path = \"Almazan_ManualData_Cleaned.xlsx\"\n",
        "\n",
        "# Load the dataset into a pandas DataFrame\n",
        "df = pd.read_excel(file_path)"
      ],
      "metadata": {
        "id": "k7v35nqRQiWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When I explored how the dataset is loaded, I found that pandas‚Äô read_excel function is a reliable and straightforward method to import Excel spreadsheets into Python. By specifying the file path \"Almazan_ManualData_Cleaned.xlsx\", I can load the contents of the Excel file into a pandas DataFrame, which is an ideal structure for data analysis because of its flexibility and rich functionality.\n",
        "\n",
        "I learned that by default, read_excel reads the first sheet in the workbook, which works well for my dataset since it‚Äôs contained in a single sheet. This method abstracts away the complexity of working directly with Excel files and provides immediate access to the data in a tabular format that integrates seamlessly with other data processing and machine learning routines.\n",
        "\n",
        "Using pandas for this step allows me to smoothly preprocess, explore, and manipulate the dataset as needed for my spam classification project."
      ],
      "metadata": {
        "id": "bUZlASoKQvRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================\n",
        "# Feature and label extraction\n",
        "# =====================\n",
        "\n",
        "# Use the \"CleanedEmail\" column as the input features (text data)\n",
        "texts = df[\"CleanedEmail\"].astype(str)\n",
        "\n",
        "# Use the \"Sentiment\" column as the labels (converted to uppercase: POSITIVE / NEGATIVE)\n",
        "labels = df[\"Sentiment\"].str.upper()"
      ],
      "metadata": {
        "id": "G0qcSkn5QpDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In reviewing this code snippet, I learned that selecting appropriate features and labels from my dataset is the foundation of any successful machine learning model.\n",
        "\n",
        "Here, I use the **\"CleanedEmail\"**\n",
        "column as my input features  the raw text data that the model will learn patterns from. To avoid unexpected issues during processing, I explicitly convert these entries to strings, ensuring the classifier works with consistent data types.\n",
        "\n",
        "For the labels or targets, I use the **\"Sentiment\"** column, which contains the categories I want the model to predict  in this case, sentiments like positive or negative. By converting these labels to uppercase (POSITIVE / NEGATIVE), I standardize the format, preventing mismatches or errors due to case sensitivity during training and evaluati\n",
        "\n",
        "I also gained insight into how transforming raw text into features typically involves techniques like Bag-of-Words or TF-IDF, which convert text into numerical representations. This step happens after feature extraction here but is a crucial part of text-based machine learning. Ensuring the labels are well prepared and normalized is equally important to define clear classes for classification."
      ],
      "metadata": {
        "id": "hYNJMF3rQ3Mp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTNIs7QKMWwZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# =====================\n",
        "# Train-test split\n",
        "# =====================\n",
        "\n",
        "# Split dataset into training (80%) and testing (20%) sets\n",
        "# random_state=42 ensures reproducibility\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    texts, labels, test_size=0.2, random_state=42\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pt5jysNfqKqp"
      },
      "source": [
        "Upon exploring this code, I learned that the` train_test_split `function from scikit-learn is a convenient and essential utility to divide data into training and testing portions. Using this function helps me separate the data so that the model can learn patterns during training and then be fairly evaluated on unseen data.\n",
        "\n",
        "Specifically, setting `test_size=0.2` means 20% of the dataset is reserved for testing, and the remaining 80% is used to train the model. This ratio is a common best practice because it balances having enough data for training while still preserving a meaningful portion to verify how well the model generalizes.\n",
        "\n",
        "The `random_state=42` parameter ensures the split is reproducible, so every time I run the code, I get the same training and testing sets. This is crucial for consistent model evaluation and debugging.\n",
        "\n",
        " I also understood that this type of data split helps prevent overfitting and ensures that performance metrics truly reflect the model‚Äôs ability to handle new inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJRzQemcMcjg",
        "outputId": "44aae350-7cc4-4baa-8ce2-e7d8bb4b8677"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# =====================\n",
        "# Build SpaCy text classifier\n",
        "# =====================\n",
        "\n",
        "# Create a blank English NLP pipeline\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Add the text classification component (\"textcat\") to the pipeline\n",
        "textcat = nlp.add_pipe(\"textcat\")\n",
        "\n",
        "# Define the possible sentiment labels (POSITIVE and NEGATIVE)\n",
        "textcat.add_label(\"POSITIVE\")\n",
        "textcat.add_label(\"NEGATIVE\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In learning about spaCy‚Äôs text classification capabilities, I discovered that starting with a blank English NLP pipeline `(spacy.blank(\"en\"))` lets me fully customize the processing steps the way I want, without loading any pre-existing models or components.\n",
        "\n",
        "Adding the **\"textcat\"** component  spaCy‚Äôs built-in text categorizer  integrates text classification functionality directly into the NLP pipeline. This component is designed to assign one or more categories (labels) to entire documents, which fits perfectly with tasks like sentiment analysis or spam detection.\n",
        "\n",
        "Defining the labels `\"POSITIVE\" and \"NEGATIVE\" `explicitly tells the text categorizer the possible categories it will predict. This setup is essential before training so the model knows which classes to focus on.\n",
        "\n",
        "From what I learned, spaCy uses various underlying architectures for text classification; it typically employs a bag-of-words model or convolutional neural networks depending on configuration. This modular pipeline design makes it straightforward to train, evaluate, and update the classifier inside the spaCy framework."
      ],
      "metadata": {
        "id": "liD65u6sRyFL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgI3CN7wMjOk"
      },
      "outputs": [],
      "source": [
        "# =====================\n",
        "# Prepare training data\n",
        "# =====================\n",
        "\n",
        "# Initialize an empty list to hold training examples\n",
        "train_data = []\n",
        "\n",
        "# Loop through each training text and its corresponding label\n",
        "for text, label in zip(X_train, y_train):\n",
        "    if label == \"POSITIVE\":\n",
        "        # If label is POSITIVE ‚Üí assign 1 to POSITIVE and 0 to NEGATIVE\n",
        "        train_data.append((text, {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}))\n",
        "    else:\n",
        "        # If label is NEGATIVE ‚Üí assign 1 to NEGATIVE and 0 to POSITIVE\n",
        "        train_data.append((text, {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLvuD0jorBnG"
      },
      "source": [
        "In looking deeper into spaCy‚Äôs training process, I discovered that the training data needs to be prepared in a very specific format. Each training example should be a tuple containing the raw text and a dictionary with a `\"cats\"` key. This key maps to another dictionary representing the categories with binary values (1 if the label applies, 0 otherwise).\n",
        "\n",
        "Since my task is binary sentiment classification with labels `\"POSITIVE\" and \"NEGATIVE\"`, I encode each label explicitly: if the label is `\"POSITIVE\"`, I assign `{\"POSITIVE\": 1, \"NEGATIVE\": 0} `and vice versa for `\"NEGATIVE\"`. This binary encoding is essential because spaCy uses these scores to calculate the loss and guide the model‚Äôs learning during training.\n",
        "\n",
        "From my research I also learned that this format is necessary for spaCy‚Äôs internal training mechanisms and helps the model understand which categories apply to each text. Consistent and accurate formatting of training examples is crucial to ensure effective learning and avoid errors when processing the data.\n",
        "\n",
        "This structured approach gave me clarity on how to bridge raw dataset labels with spaCy's requirements, giving a solid foundation for training my text classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBZWJUciMlVY",
        "outputId": "f441045a-3bc2-4393-a257-d56e555bae21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30 - Losses: {'textcat': 12.154008135199547}\n",
            "Epoch 2/30 - Losses: {'textcat': 12.131330385804176}\n",
            "Epoch 3/30 - Losses: {'textcat': 11.290689896792173}\n",
            "Epoch 4/30 - Losses: {'textcat': 8.847976505756378}\n",
            "Epoch 5/30 - Losses: {'textcat': 7.077652743668295}\n",
            "Epoch 6/30 - Losses: {'textcat': 3.0277781209827026}\n",
            "Epoch 7/30 - Losses: {'textcat': 2.6571966178970556}\n",
            "Epoch 8/30 - Losses: {'textcat': 0.07026756973075438}\n",
            "Epoch 9/30 - Losses: {'textcat': 0.0551882345254458}\n",
            "Epoch 10/30 - Losses: {'textcat': 0.6630007952918034}\n",
            "Epoch 11/30 - Losses: {'textcat': 1.9386224136115533}\n",
            "Epoch 12/30 - Losses: {'textcat': 0.012346977159501839}\n",
            "Epoch 13/30 - Losses: {'textcat': 0.5295619889769305}\n",
            "Epoch 14/30 - Losses: {'textcat': 0.006729364867999449}\n",
            "Epoch 15/30 - Losses: {'textcat': 0.9989663168503519}\n",
            "Epoch 16/30 - Losses: {'textcat': 4.0961208454211415e-05}\n",
            "Epoch 17/30 - Losses: {'textcat': 0.0013649537230135277}\n",
            "Epoch 18/30 - Losses: {'textcat': 0.7766843458570777}\n",
            "Epoch 19/30 - Losses: {'textcat': 0.5045072659788432}\n",
            "Epoch 20/30 - Losses: {'textcat': 6.418681381587023e-05}\n",
            "Epoch 21/30 - Losses: {'textcat': 0.0001866606000581556}\n",
            "Epoch 22/30 - Losses: {'textcat': 0.004677263927873001}\n",
            "Epoch 23/30 - Losses: {'textcat': 0.9248532679372106}\n",
            "Epoch 24/30 - Losses: {'textcat': 0.001551523443456833}\n",
            "Epoch 25/30 - Losses: {'textcat': 0.001974274893064447}\n",
            "Epoch 26/30 - Losses: {'textcat': 9.84730154752557e-05}\n",
            "Epoch 27/30 - Losses: {'textcat': 0.0017262350148259523}\n",
            "Epoch 28/30 - Losses: {'textcat': 6.290685937402126e-06}\n",
            "Epoch 29/30 - Losses: {'textcat': 1.5962314529014375e-07}\n",
            "Epoch 30/30 - Losses: {'textcat': 8.141072871824335e-05}\n"
          ]
        }
      ],
      "source": [
        "# =====================\n",
        "# Training function\n",
        "# =====================\n",
        "\n",
        "def train_model(data, n_iter=30):\n",
        "    # Shuffle training data before each run\n",
        "    random.shuffle(data)\n",
        "\n",
        "    # Initialize the model optimizer\n",
        "    optimizer = nlp.begin_training()\n",
        "\n",
        "    # Loop through the specified number of epochs (default = 30)\n",
        "    for epoch in range(n_iter):\n",
        "        losses = {}\n",
        "        for text, annotations in data:\n",
        "            # Convert training text and labels into SpaCy Example format\n",
        "            doc = nlp.make_doc(text)\n",
        "            example = Example.from_dict(doc, annotations)\n",
        "\n",
        "            # Update the model weights, applying dropout to prevent overfitting\n",
        "            nlp.update([example], drop=0.5, losses=losses)\n",
        "\n",
        "        # Print losses for monitoring progress\n",
        "        print(f\"Epoch {epoch+1}/{n_iter} - Losses: {losses}\")\n",
        "\n",
        "# Train the model using prepared training data\n",
        "train_model(train_data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I02fQ-ecrhZb"
      },
      "source": [
        "From my point of view, I researched spaCy‚Äôs training process and the role of dropout regularization to ensure that the training function I implemented follows best practices.\n",
        "\n",
        "I understand that the training loop is fundamental for the model to learn by updating its weights repetitively over multiple epochs. I made sure to shuffle the training data before each run because I learned that this helps prevent the model from memorizing the order of the data, which promotes better generalization.\n",
        "\n",
        "Using `nlp.update()` with spaCy‚Äôs Example objects is the recommended way to train the model, as it properly aligns the raw text and annotations for effective learning. Including dropout with a rate of **0.5** is important because, from my research, I know that dropout randomly disables parts of the neural network during training to reduce overfitting and encourage more robust feature learning.\n",
        "\n",
        "I decided to increase the number of training iterations to 30 based on my findings that more epochs give the model additional opportunities to capture complex patterns, thereby improving performance though I am mindful of the balance to avoid overfitting.\n",
        "\n",
        "Reference used : https://www.lunartech.ai/blog/mastering-dropout-in-neural-networks-a-comprehensive-guide-to-preventing-overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cP4cR8mMpgA",
        "outputId": "f6d8ca68-f0ed-4e97-8bbe-a2e208ff6234"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a text for sentiment analysis: Covid19\n",
            "\n",
            "Sentiment Prediction:\n",
            "{'POSITIVE': 0.999903678894043, 'NEGATIVE': 9.628659609006718e-05}\n",
            "\n",
            "Evaluation Metrics:\n",
            "Accuracy : 0.9166666666666666\n",
            "Recall   : 0.8\n",
            "F1 Score : 0.8888888888888888\n"
          ]
        }
      ],
      "source": [
        "# =====================\n",
        "# User input test\n",
        "# =====================\n",
        "\n",
        "# Prompt the user to enter custom text for sentiment analysis\n",
        "user_input = input(\"Enter a text for sentiment analysis: \")\n",
        "\n",
        "# Pass the input text into the prediction function\n",
        "prediction = predict_sentiment(user_input)\n",
        "\n",
        "# Display the predicted sentiment scores\n",
        "print(\"\\nSentiment Prediction:\")\n",
        "print(prediction)\n",
        "\n",
        "\n",
        "# =====================\n",
        "# Evaluation\n",
        "# =====================\n",
        "\n",
        "# Create an empty list to store predicted labels\n",
        "y_pred = []\n",
        "\n",
        "# Loop through test set and classify each text\n",
        "for text in X_test:\n",
        "    doc = nlp(text)\n",
        "    if doc.cats[\"POSITIVE\"] > doc.cats[\"NEGATIVE\"]:\n",
        "        y_pred.append(\"POSITIVE\")\n",
        "    else:\n",
        "        y_pred.append(\"NEGATIVE\")\n",
        "\n",
        "# Print evaluation metrics using the test set\n",
        "print(\"\\nEvaluation Metrics:\")\n",
        "print(\"Accuracy :\", accuracy_score(y_test, y_pred))\n",
        "print(\"Recall   :\", recall_score(y_test, y_pred, pos_label=\"POSITIVE\"))\n",
        "print(\"F1 Score :\", f1_score(y_test, y_pred, pos_label=\"POSITIVE\"))\n",
        "\n",
        "\n",
        "# =====================\n",
        "# Function to predict sentiment\n",
        "# =====================\n",
        "\n",
        "# This function takes a text input, processes it through the model,\n",
        "# and returns sentiment scores for POSITIVE and NEGATIVE\n",
        "def predict_sentiment(text):\n",
        "    doc = nlp(text)\n",
        "    return doc.cats\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After implementing this interactive user input section, I realized how powerful spaCy makes it to apply my trained sentiment classifier to any new text on the fly. The predict_sentiment function processes the raw input through the nlp pipeline and returns category scores, which reflect spaCy‚Äôs confidence in the text belonging to each class.\n",
        "\n",
        "Prompting the user to provide any custom text allows me to test the classifier real-time in an intuitive way, seeing immediate results of POSITIVE or NEGATIVE sentiment for arbitrary inputs.\n",
        "\n",
        "Regarding evaluation, I use the test set predictions to comprehensively measure performance using scikit-learn‚Äôs metrics. The accuracy score.The accuracy score (~91.7%) indicates that most texts are correctly classified overall. More importantly, the recall (~80%)  for the POSITIVE label shows that the model effectively identifies a large portion of actual positive examples, which is vital for many practical applications where missing positives would be costly."
      ],
      "metadata": {
        "id": "07itF-WiWr3J"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn9zDjRLr24J"
      },
      "source": [
        "\n",
        "The F1 score (~88.9%) balances precision and recall, reflecting a strong ability of the model to correctly classify sentiment with both completeness and correctness. I found these results encouraging, demonstrating that my spaCy model is reliable and effective for sentiment analysis.\n",
        "\n",
        "This hands-on approach combined with quantitative evaluation helped me better appreciate the classifier‚Äôs strengths and potential areas for improvement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCJlk6KrDftl"
      },
      "source": [
        "# NLP Application: Part of Speech Tagging\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "yZS1VVd0FZ4s"
      },
      "outputs": [],
      "source": [
        "# =====================\n",
        "# Import Required Libraries\n",
        "# =====================\n",
        "\n",
        "import ast  # For safely evaluating and parsing string-based Python expressions\n",
        "import spacy  # Core NLP framework for tokenization, POS tagging, and text classification\n",
        "import pandas as pd  # Data manipulation and dataset handling\n",
        "# Import evaluation metrics from sklearn to measure model performance\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSIqDg6CFjE_"
      },
      "source": [
        "The ast module is useful because it allows me to safely evaluate and parse string-based Python expressions. This can be handy when dealing with data formats or configurations stored as strings that need to be converted into Python objects without security risks.\n",
        "\n",
        "spaCy is my core NLP framework. From my research, I learned that spaCy excels at efficient tokenization, part-of-speech tagging, dependency parsing, and it also supports robust pipelines for training text classification models. Its performance and scalability make it ideal for processing and analyzing natural language data at scale.\n",
        "\n",
        "pandas is essential for managing and manipulating my datasets. It offers versatile data structures like DataFrames that simplify reading, cleaning, and organizing data before feeding it into a model.\n",
        "\n",
        "From scikit-learn, I selectively import evaluation metrics like accuracy, precision, recall, and F1 score. These metrics provide detailed insights into how well my model is performing. Accuracy measures general correctness, precision indicates the reliability of positive predictions, recall shows the model‚Äôs sensitivity in finding actual positives, and F1 score balances precision and recall for a holistic performance view."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "9txq0zNUGSuA"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Hyperparameters\n",
        "# ----------------------------\n",
        "HYPERPARAMS = {\n",
        "    \"batch_size\": 32,                  # Process data in batches of 32 for efficiency\n",
        "    \"disable_components\": [\"ner\"],     # Disable Named Entity Recognition (NER) to speed up processing\n",
        "    \"max_length\": 2000000              # Maximum text length allowed for processing\n",
        "}\n",
        "\n",
        "# ----------------------------\n",
        "# Load Pre-trained spaCy Model\n",
        "# ----------------------------\n",
        "# Load the small English model while disabling NER (based on hyperparameters)\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=HYPERPARAMS[\"disable_components\"])\n",
        "# Set the maximum text length for the pipeline (useful for longer inputs)\n",
        "nlp.max_length = HYPERPARAMS[\"max_length\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1N50LrrGc4Z"
      },
      "source": [
        "Through research on spaCy hyperparameters, I found that the choices I made are quite aligned with common practices:\n",
        "\n",
        "- Setting a batch size like 32 is a typical way to balance memory efficiency and training speed. Processing data in batches allows models to update weights more effectively compared to single samples, as well as utilizing computational resources better.\n",
        "\n",
        "- Disabling components like \"ner\" (Named Entity Recognition) during certain processing steps is recommended to speed up pipeline performance when those components are unnecessary for the current task. This helps reduce computational overhead and resource use, especially during data preprocessing or when focusing on other model parts.\n",
        "\n",
        "- Setting a maximum text length (max_length) to a high value such as 2,000,000 helps avoid truncation of long documents during processing. In spaCy, this parameter ensures that large texts can be processed without errors, which is important when working with extensive datasets.\n",
        "\n",
        "Reference Used : https://stackoverflow.com/questions/74181750/a-checklist-for-spacy-optimization\n",
        "\n",
        "For the the spaCy pre-trained model \"en_core_web_sm\" that I loaded, and found out that this small English model is optimized for CPU and includes components like tokenizer, tagger, parser, lemmatizer, and named entity recognizer (NER), though I chose to disable NER based on my hyperparameters.\n",
        "\n",
        "I also learned that setting` nlp.max_length` to a large number lets me process very long texts without errors, which is valuable for working with extensive datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "wWMwTmQ1G-AA"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Load Dataset\n",
        "# ----------------------------\n",
        "# Read the cleaned tweets dataset from an Excel file\n",
        "df = pd.read_excel(\"cleaned_tweets.xlsx\")\n",
        "\n",
        "# Extract the raw cleaned tweet text\n",
        "texts = df[\"Clean_Tweet\"]\n",
        "\n",
        "# Convert tokenized tweet strings back into Python lists using ast.literal_eval\n",
        "tokenized_tweets = df[\"Tokenized_Tweet\"].apply(lambda x: ast.literal_eval(x))\n",
        "\n",
        "# Convert POS tag strings back into Python lists using ast.literal_eval\n",
        "true_pos_tags = df[\"POS_Tags\"].apply(lambda x: ast.literal_eval(x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dM9o0cfQHMNI"
      },
      "source": [
        "In digging into this step, I learned that `pandas‚Äô` read_excel function lets me easily import the cleaned tweets dataset stored in an Excel file `\"cleaned_tweets.xlsx\"` into a pandas DataFrame. This provides a structured, tabular view of my dataset, making subsequent analysis much easier.\n",
        "\n",
        "I extracted the `\"Clean_Tweet\"` column to get the raw text of each tweet, keeping it ready for any text-based processing, like cleaning or tokenization.\n",
        "\n",
        "Notably, two other columns `\"Tokenized_Tweet\" and \"POS_Tags\"` contain information stored as strings representing lists. To convert these string representations back into actual Python list objects, I used the ast.literal_eval function safely within a lambda. This approach protects against arbitrary code execution, unlike using eval, while restoring the data to the right format for NLP processing.\n",
        "\n",
        "This detailed preprocessing ensures that both the text and structured linguistic information like part-of-speech tags are available in the correct formats, forming a solid base for any downstream NLP tasks or model training I plan to do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBAaGd-YHRcV"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# POS Analysis Function\n",
        "# ----------------------------\n",
        "def analyze_text(text):\n",
        "    # Process the input text using the SpaCy NLP pipeline\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Extract each token and its corresponding POS tag into a list of tuples\n",
        "    pos_list = [(token.text, token.pos_) for token in doc]\n",
        "\n",
        "    # Return the list of (word, POS) pairs\n",
        "    return pos_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6LASpMhHroI"
      },
      "source": [
        "My function analyze_text performs exactly this: it takes the input string, processes it to create a Doc object, then collects all tokens along with their universal POS tags (token.pos_) into a list of pairs. These tags reflect the syntactic category of each token in a simplified, standardized form.\n",
        "\n",
        "In studying part-of-speech (POS) tagging with spaCy, I learned it‚Äôs a powerful tool to understand the grammatical structure of text by labeling each word with its role, such as noun, verb, adjective, etc. SpaCy‚Äôs POS tagger works by processing the input text through the NLP pipeline, which tokenizes the text and assigns linguistic annotations.\n",
        "\n",
        "My function analyze_text performs exactly this: it takes the input string, processes it to create a Doc object, then collects all tokens along with their universal POS tags` (token.pos_) `into a list of pairs. These tags reflect the syntactic category of each token in a simplified, standardized form.\n",
        "\n",
        "From my research, I found these tags are statistically assigned based on a trained model that considers the context surrounding each word, making the tagging accurate and context-aware.\n",
        "\n",
        "This POS extraction lays the groundwork for deeper text understanding and helps in many NLP tasks like parsing, entity recognition, and feature extraction for machine learning.\n",
        "\n",
        "By implementing this function, I got hands-on experience with spaCy‚Äôs linguistic features and how to programmatically access detailed syntactic information from text.\n",
        "\n",
        "Reference Used : https://www.askpython.com/python/examples/pos-tagging-in-nlp-using-spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZ0GwWnpH9ka"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Evaluate on Dataset\n",
        "# ----------------------------\n",
        "\n",
        "# Initialize lists to hold the true POS tags and predicted POS tags\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "# Loop through each tokenized tweet and its corresponding gold-standard POS tags\n",
        "for tokens, gold_tags in zip(tokenized_tweets, true_pos_tags):\n",
        "    # Reconstruct the sentence by joining tokens with spaces\n",
        "    text = \" \".join(tokens)\n",
        "\n",
        "    # Run the SpaCy POS tagger on the reconstructed text\n",
        "    pred_pairs = analyze_text(text)  # returns [(word, POS), (word, POS), ...]\n",
        "\n",
        "    # Extract only the POS tags from the prediction results\n",
        "    pred_tags = [pos for _, pos in pred_pairs]\n",
        "\n",
        "    # Ensure alignment between gold and predicted tags\n",
        "    if len(pred_tags) == len(gold_tags):\n",
        "        # Append gold-standard tags and predicted tags for evaluation\n",
        "        y_true.extend(gold_tags)\n",
        "        y_pred.extend(pred_tags)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZBnAG8WIDET"
      },
      "source": [
        "In this evaluation step, I loop through each tokenized tweet and its corresponding gold-standard part-of-speech (POS) tags to assess how well my spaCy POS tagger performs.\n",
        "\n",
        "By reconstructing the tweet text from tokens and running my `analyze_text` function, I generate predicted POS tags which I then extract into a list for comparison.\n",
        "\n",
        "I make sure to only compare when the predicted and gold tags align in length, ensuring a fair token-by-token evaluation.\n",
        "\n",
        "From my research, the standard evaluation metric for POS tagging is accuracy, which measures the proportion of correctly identified POS tags relative to the total tags. Although other metrics like precision, recall, and F1 score can be applied to individual POS tags, the overall accuracy gives a straightforward measure of tagging quality over the dataset.\n",
        "\n",
        "This approach of aggregating true and predicted tags into lists prepares me to compute these metrics and understand how effectively the model grasps the syntactic structure of the tweets, guiding improvements or further training if needed.\n",
        "\n",
        "Reference used : https://fiveable.me/natural-language-processing/unit-3/part-of-speech-tagging/study-guide/PhpMnmXgmab6yQKe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34kWDK7rIQX2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00f460b8-793a-4f5e-8ebd-9bf7bed14452"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Enter a text for POS tagging analysis: for those who arent struggling please consider donating to a food bank or a nonprofit the demand for these services will increase as covid19 impacts jobs and peoples way of life\n",
            "\n",
            "Tokens and POS Tags:\n",
            "[('for', 'ADP'), ('those', 'PRON'), ('who', 'PRON'), ('are', 'AUX'), ('nt', 'PART'), ('struggling', 'VERB'), ('please', 'INTJ'), ('consider', 'VERB'), ('donating', 'VERB'), ('to', 'ADP'), ('a', 'DET'), ('food', 'NOUN'), ('bank', 'NOUN'), ('or', 'CCONJ'), ('a', 'DET'), ('nonprofit', 'ADJ'), ('the', 'DET'), ('demand', 'NOUN'), ('for', 'ADP'), ('these', 'DET'), ('services', 'NOUN'), ('will', 'AUX'), ('increase', 'VERB'), ('as', 'ADP'), ('covid19', 'ADJ'), ('impacts', 'NOUN'), ('jobs', 'NOUN'), ('and', 'CCONJ'), ('peoples', 'NOUN'), ('way', 'NOUN'), ('of', 'ADP'), ('life', 'NOUN')]\n",
            "\n",
            "Evaluation Metrics:\n",
            "Hyperparameters: {'batch_size': 32, 'disable_components': ['ner'], 'max_length': 2000000}\n",
            "Accuracy : 0.8156\n",
            "Precision: 0.7242\n",
            "Recall   : 0.7527\n",
            "F1-score : 0.7287\n"
          ]
        }
      ],
      "source": [
        "# ----------------------------\n",
        "# User Test\n",
        "# ----------------------------\n",
        "# Ask the user to input a sentence for testing the POS tagger\n",
        "user_input = input(\"\\nEnter a text for POS tagging analysis: \")\n",
        "\n",
        "# Process the user input with the SpaCy NLP pipeline\n",
        "user_doc = nlp(user_input)\n",
        "\n",
        "# Display the tokens along with their predicted POS tags\n",
        "print(\"\\nTokens and POS Tags:\")\n",
        "print([(token.text, token.pos_) for token in user_doc])\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Evaluation Metrics\n",
        "# ----------------------------\n",
        "# Only evaluate if we have valid gold-standard labels\n",
        "if len(y_true) > 0:\n",
        "    # Calculate standard evaluation metrics for POS tagging\n",
        "    accuracy  = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "    recall    = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "    f1        = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "\n",
        "    # Print results along with the hyperparameters used\n",
        "    print(\"\\nEvaluation Metrics:\")\n",
        "    print(\"Hyperparameters:\", HYPERPARAMS)\n",
        "    print(f\"Accuracy : {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall   : {recall:.4f}\")\n",
        "    print(f\"F1-score : {f1:.4f}\")\n",
        "else:\n",
        "    # Safety fallback in case dataset format is inconsistent\n",
        "    print(\"\\nNo valid rows for evaluation. Double-check dataset formatting.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDaM6vJVIih-"
      },
      "source": [
        "In this part, I added an interactive element that lets users input any sentence and immediately see the SpaCy POS tagger‚Äôs output. By processing the input with `nlp(user_input)`, I generate tokenized words enriched with linguistic features, specifically their predicted part-of-speech tags. Printing out each token alongside its POS tag provides transparent insight into how the model interprets the syntax of the input sentence.\n",
        "\n",
        "For evaluating the POS tagger's performance on the dataset, I aggregate true POS tags `(y_true)` and predicted tags `(y_pred)` and then compute key evaluation metrics using scikit-learn. These metrics include:\n",
        "\n",
        "- Accuracy, measuring the overall percentage of correctly predicted tags.\n",
        "\n",
        "- Precision, Recall, and F1-score calculated as macro-averages, giving an overall sense of tagging quality across all POS categories.\n",
        "\n",
        "The inclusion of `zero_division=0` ensures numeric stability in case some POS tags don't appear in predictions or ground truth.\n",
        "\n",
        "I also print out the hyperparameters used during training to keep track of the experimental setup, important for reproducibility and performance tuning.\n",
        "\n",
        "- Accuracy (~81.56%) means that over 81% of the tokens in the test dataset were correctly assigned their part-of-speech tags by the model. This indicates strong overall tagging performance but leaves some room for improvement.\n",
        "\n",
        "- Precision (~72.42%) reflects that when the model predicts a POS tag, about 72% of those predictions are correct on average across all POS categories. This shows the model performs reliably in identifying POS tags it predicts.\n",
        "\n",
        "- Recall (~75.27%) suggests the model successfully finds about 75% of all actual POS tags present in the gold standard, indicating good coverage in identifying correct tags.\n",
        "\n",
        "- F1-score (~72.87%) balances precision and recall, giving a comprehensive sense that the model‚Äôs tagging ability is solid but not perfect."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQ8RfMg_R-Ji"
      },
      "source": [
        "# NLP Application: Text Summarizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrZYnbsXsjbQ",
        "outputId": "c2232e96-d23a-4c25-f441-a6eeccfa1f56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from rouge) (1.17.0)\n",
            "Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n",
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (4.67.1)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=9999f51233d3e84cdb34584f6b2a95c35b25f7e77d95b8d6f594a32c9a0e473a\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install rouge\n",
        "!pip install rouge-score\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "are commands used in a Python environment (like Jupyter Notebook) to install two different Python packages named \"rouge\" and \"rouge-score\" using pip, which is the standard package manager for Python.\n",
        "\n",
        "From my research, here's what I understood:\n",
        "\n",
        "\n",
        "*   The rouge-score package is a Python library designed to calculate ROUGE metrics, which are used to evaluate the quality of summaries generated by models by comparing them to human-written reference summaries. It's especially popular in natural language processing tasks like text summarization. I tried install this package with pip install rouge-score, i get tools that can compute various ROUGE scores such as ROUGE-1, ROUGE-2, and ROUGE-L, which measure overlap in n-grams or longest common subsequences between two texts.\n",
        "\n",
        "\n",
        "*   The rouge package is a different Python library that also provides ROUGE metric calculations, but it is independent from the official ROUGE script and has its own implementation. I install it with `pip install rouge`.\n",
        "\n",
        "\n",
        "Reference used: https://www.freecodecamp.org/news/what-is-rouge-and-how-it-works-for-evaluation-of-summaries-e059fb8ac840/\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "atC3Zx1cDQRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from rouge_score import rouge_scorer  # pip install rouge-score"
      ],
      "metadata": {
        "id": "LvzvAlqIzXLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   When I write import spacy, I am bringing in the spaCy library. spaCy is a popular Python library that helps me do Natural Language Processing (NLP). That means it can help me work with human language data, like breaking text into words (tokenizing), finding parts of speech, recognizing names and places, and other text analysis tasks. To make spaCy work fully, I usually need to download a language model like en_core_web_sm which contains the rules and data spaCy uses to understand English text.\n",
        "*   import pandas as pd means I'm importing pandas, which is a library for handling data in tables like spreadsheets. I use pandas a lot to read, organize, and manipulate data easily, especially if I want to analyze or prepare text data stored in CSV or Excel files.\n",
        "\n",
        "*   from collections import Counter is a way to import a useful Python tool called Counter. Counter helps me count how many times each item appears in a list or collection. For example, if I have a bunch of words, I can use Counter to find out the frequency of each word quickly.\n",
        "*   Finally, from rouge_score import rouge_scorer means I am importing the rouge_scorer module from the rouge_score package, which I installed earlier. This lets me calculate ROUGE scores, metrics used mainly to evaluate the quality of summaries by comparing automatically generated summaries against reference summaries. The comment reminds me to install rouge-score with pip if it‚Äôs not installed already.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xw0YJq25Exn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Load model\n",
        "# ===============================\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "7h2Q3F6PzZCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* This line of code is loading a model inside the spaCy library. A model in spaCy is like a pre-trained tool that knows how to understand and analyze English text. It has learned patterns in language, such as grammar, word meanings, and sentence structure.\n",
        "\n",
        "* The part \"en_core_web_sm\" is the name of a specific spaCy language model for English. It's a \"small\" size model that has enough data and rules to do basic natural language processing tasks like tokenization (breaking text down into words), part-of-speech tagging (telling nouns, verbs, etc.), and named entity recognition (finding names, places, dates in text).\n"
      ],
      "metadata": {
        "id": "Ev4ftN5hFVlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Summarization of text\n",
        "# ===============================\n",
        "def summarize(text, n_sentences=2):\n",
        "    doc = nlp(str(text))\n",
        "    sentence_scores = Counter()\n",
        "\n",
        "    # Score sentences based on token frequency\n",
        "    for sent in doc.sents:\n",
        "        for token in sent:\n",
        "            if not token.is_stop and not token.is_punct:\n",
        "                sentence_scores[sent] += 1\n",
        "    # Select top N sentences\n",
        "    top_sentences = [sent.text for sent, score in sentence_scores.most_common(n_sentences)]\n",
        "    return \" \".join(top_sentences)"
      ],
      "metadata": {
        "id": "qz85cbwo1xOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From my research, here's what I understood\n",
        "\n",
        "* The **rouge-score** package is a Python library designed to calculate ROUGE\n",
        "metrics, which are used to evaluate the quality of summaries generated by models by comparing them to human-written reference summaries. It's especially popular in natural language processing tasks like text summarization. When you install this package with` pip install rouge-score`, you get tools that can compute various ROUGE scores such as ROUGE-1, ROUGE-2, and ROUGE-L, which measure overlap in n-grams or longest common subsequences between two texts.\n",
        "\n",
        "* The **rouge package** is a different Python library that also provides ROUGE metric calculations, but it is independent from the official ROUGE script and has its own implementation. You install it with `pip install rouge.`\n",
        "\n",
        "Reference used : https://www.freecodecamp.org/news/what-is-rouge-and-how-it-works-for-evaluation-of-summaries-e059fb8ac840/"
      ],
      "metadata": {
        "id": "K913t-iQGDRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Load Dataset\n",
        "# ===============================\n",
        "file_path = \"cleaned_tweets.xlsx\"   # ensure file is in the same directory\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# ===============================\n",
        "# Apply Summarization to Dataset\n",
        "# ===============================\n",
        "df[\"Summary\"] = df[\"Clean_Tweet\"].apply(lambda x: summarize(x, n_sentences=1))"
      ],
      "metadata": {
        "id": "EwdK5rk8zj_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I found that using pandas' read_excel function is a straightforward and efficient way to load data from Excel files directly into a DataFrame, which is well-suited for data manipulation tasks. This function supports multiple Excel formats (like .xls, .xlsx) and automatically interprets the sheet structure, making it easy to work with tabular data. For this to work smoothly, the Excel file needs to be accessible in the current working directory or specify the full path\n",
        "\n",
        "By using pandas' `apply` method on a DataFrame column, I can apply any function in this case, a `summarize` function‚Äîto each row's content. This is powerful for processing text data row-wise without writing explicit loops. The lambda function here compresses each cleaned tweet into a single summarized sentence, which I learned helps reduce text complexity while retaining key information for analysis."
      ],
      "metadata": {
        "id": "lFyTBBr4LHPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# User input for testing\n",
        "# ===============================\n",
        "user_text = input(\"\\nEnter the text you want to summarize: \")\n",
        "summary = summarize(user_text, n_sentences=1)\n",
        "print(\"\\nSummary:\")\n",
        "print(summary)\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# ROUGE Evaluation\n",
        "# ===============================\n",
        "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "\n",
        "def normalize(text):\n",
        "    return str(text).lower().strip()\n",
        "\n",
        "rouge1_scores, rouge2_scores, rougel_scores = [], [], []\n",
        "\n",
        "for ref, hyp in zip(df[\"Clean_Tweet\"], df[\"Reference_Summary\"]):\n",
        "    scores = scorer.score(normalize(ref), normalize(hyp))\n",
        "    rouge1_scores.append(scores[\"rouge1\"].fmeasure)\n",
        "    rouge2_scores.append(scores[\"rouge2\"].fmeasure)\n",
        "    rougel_scores.append(scores[\"rougeL\"].fmeasure)\n",
        "\n",
        "print(\"\\nEvaluation Metrics (Average):\")\n",
        "print(f\"ROUGE-1 F1: {sum(rouge1_scores)/len(rouge1_scores):.4f}\")\n",
        "print(f\"ROUGE-2 F1: {sum(rouge2_scores)/len(rouge2_scores):.4f}\")\n",
        "print(f\"ROUGE-L F1: {sum(rougel_scores)/len(rougel_scores):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUWiH0sBzmzX",
        "outputId": "83acaea1-d6ed-431d-d01d-e6398bf314d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Enter the text you want to summarize: advice talk to your neighbours family to exchange phone numbers create contact list with phone numbers of neighbours schools employer chemist gp set up online shopping accounts if poss adequate supplies of regular meds but not over order\n",
            "\n",
            "Summary:\n",
            "advice talk to your neighbours family to exchange phone numbers create contact list with phone numbers of neighbours schools employer chemist gp set up online shopping accounts if poss adequate supplies of regular meds but not over order\n",
            "\n",
            "Evaluation Metrics (Average):\n",
            "ROUGE-1 F1: 0.8636\n",
            "ROUGE-2 F1: 0.8289\n",
            "ROUGE-L F1: 0.8636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**User Input and Summarization**\n",
        "\n",
        "I discovered that `input()` function provides a simple interactive way to capture user input from the console, which can then be processed dynamically. Here, the entered text is passed to a `summarize` function which condenses the input into one sentence. This real-time summarization approach allows quick testing of how well the summarization model works on arbitrary text snippets.\n",
        "\n",
        "**ROUGE Evaluation**\n",
        "\n",
        "For evaluating summarization quality, I learned the ROUGE metric is widely used in natural language processing to compare generated summaries against reference summaries. The rouge_scorer.RougeScorer class computes scores such as ROUGE-1, ROUGE-2, and ROUGE-L, which measure overlaps of unigrams, bigrams, and longest common subsequences respectively. Normalizing text by lowercasing and stripping whitespace helps ensure consistent scoring. Calculating average F1 scores over the dataset provides an overall metric to assess the summarization model's effectiveness.\n",
        "\n",
        "\n",
        "The scores here ROUGE-1 F1: 0.8636, ROUGE-2 F1: 0.8289, and ROUGE-L F1: 0.8636  indicate strong performance by the summarization model. These values suggest that the generated summaries not only capture much of the essential vocabulary but also maintain coherent phrase structures close to the reference summaries, which is a positive sign of good summarization quality."
      ],
      "metadata": {
        "id": "xN_woKkGLeRi"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "OS6-4moH11Na"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}